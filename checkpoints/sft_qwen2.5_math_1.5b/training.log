dataset len: 3735
dataset len: 414
Normalize batch size by dp 1
Using sequence parallel size: 1
Using remove padding: True
Using FSDP rank 0 and size 1 for data distribution
/u/fvc9ch/nlp_research/VLLM-math-generation/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
Skipping monkey patch for Qwen2ForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[2026-02-19 11:37:34,461][liger_kernel.transformers.monkey_patch][INFO] - Applying Liger kernels to model instance with model type: qwen2 with kwargs: {}
functools.partial(<function _or_policy at 0x1480e73832e0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x1480e73831a0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
NCCL version 2.27.5+cuda12.9
Number of steps/epoch 14, number of epochs 10, total number of steps 140
Generation probe: 5 prompts pre-tokenized for response length measurement
Found checkpoint: %s ./checkpoints/sft_qwen2.5_math_1.5b/global_step_14
Found latest checkpoint: ./checkpoints/sft_qwen2.5_math_1.5b/global_step_14 (step 14)
[2026-02-19 11:37:37,282][/u/fvc9ch/nlp_research/verl/verl/utils/checkpoint/fsdp_checkpoint_manager.py][INFO] - [Rank 0] Loaded model from ./checkpoints/sft_qwen2.5_math_1.5b/global_step_14/model_world_size_1_rank_0.pt
/u/fvc9ch/nlp_research/VLLM-math-generation/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
{'optim': {'_target_': 'verl.workers.config.FSDPOptimizerConfig', 'optimizer': 'AdamW', 'optimizer_impl': 'torch.optim', 'lr': 1e-05, 'lr_warmup_steps_ratio': 0.1, 'total_training_steps': -1, 'weight_decay': 0.01, 'lr_warmup_steps': -1, 'betas': [0.9, 0.95], 'clip_grad': 1.0, 'min_lr_ratio': 0.0, 'num_cycles': 0.5, 'lr_scheduler_type': 'constant', 'warmup_style': None, 'override_optimizer_config': None, 'lr_scheduler': 'cosine'}, 'data': {'train_batch_size': 256, 'micro_batch_size': None, 'micro_batch_size_per_gpu': 1, 'train_files': './data/sft/train.parquet', 'val_files': './data/sft/val.parquet', 'train_max_samples': -1, 'val_max_samples': -1, 'prompt_key': 'prompt', 'response_key': 'generated_solution', 'prompt_dict_keys': None, 'response_dict_keys': None, 'multiturn': {'enable': False, 'messages_key': 'messages', 'tools_key': 'tools', 'enable_thinking_key': 'enable_thinking'}, 'max_length': 16384, 'truncation': 'error', 'balance_dp_token': False, 'chat_template': None, 'custom_cls': {'path': None, 'name': None}, 'use_shm': False, 'apply_chat_template_kwargs': {}}, 'model': {'partial_pretrain': './Qwen2.5-Math-1.5B', 'use_shm': False, 'fsdp_config': {'model_dtype': 'bf16', 'wrap_policy': {'min_num_params': 0}, 'cpu_offload': False, 'offload_params': False}, 'external_lib': None, 'enable_gradient_checkpointing': True, 'trust_remote_code': False, 'lora_rank': 0, 'lora_alpha': 16, 'target_modules': 'all-linear', 'use_liger': True, 'attn_implementation': 'flash_attention_2', 'strategy': 'fsdp2'}, 'ulysses_sequence_parallel_size': 1, 'use_remove_padding': True, 'trainer': {'default_local_dir': './checkpoints/sft_qwen2.5_math_1.5b', 'default_hdfs_dir': None, 'project_name': 'math-sft', 'experiment_name': 'math-sft-20260219_113726', 'total_epochs': 10, 'total_training_steps': None, 'logger': ['console', 'wandb'], 'seed': 1, 'save_freq': 70, 'test_freq': 1, 'nnodes': 1, 'n_gpus_per_node': 8, 'max_ckpt_to_keep': None, 'generation_probe_size': 5, 'generation_probe_max_new_tokens': None, 'resume_mode': 'auto', 'resume_from_path': None, 'checkpoint': {'save_contents': ['model', 'hf_model'], 'load_contents': '${trainer.checkpoint.save_contents}'}, 'device': 'cuda'}}
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /u/fvc9ch/.netrc.
wandb: Currently logged in as: shrubsdaone (shubs-university-of-virginia) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run 87tth54e
wandb: Tracking run with wandb version 0.25.0
wandb: Run data is saved locally in /u/fvc9ch/nlp_research/VLLM-math-generation/wandb/run-20260219_113738-87tth54e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run math-sft-20260219_113726
wandb: ‚≠êÔ∏è View project at https://wandb.ai/shubs-university-of-virginia/math-sft
wandb: üöÄ View run at https://wandb.ai/shubs-university-of-virginia/math-sft/runs/87tth54e
Epoch 2/10:   0%|          | 0/14 [00:00<?, ?it/s]/u/fvc9ch/nlp_research/VLLM-math-generation/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/u/fvc9ch/nlp_research/VLLM-math-generation/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:659: UserWarning: pin_memory_device is deprecated, the current accelerator will be used as the device,ignore pin_memory_device='cuda'.
  warnings.warn(
Epoch 2/10:   0%|          | 0/14 [00:00<?, ?it/s]
Epoch 3/10:   0%|          | 0/14 [00:00<?, ?it/s]step:15 - train/loss:0.7085740566253662 - train/lr(1e-3):0.0007142857142857143 - train/time(s):154.67765545845032 - train/avg_response_length:2896.94921875 - train/response_tokens_per_step:741619.0
/u/fvc9ch/nlp_research/VLLM-math-generation/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/u/fvc9ch/nlp_research/VLLM-math-generation/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:659: UserWarning: pin_memory_device is deprecated, the current accelerator will be used as the device,ignore pin_memory_device='cuda'.
  warnings.warn(
step:15 - val/loss:0.6990448236465454 - val/avg_generated_length:665.6
/u/fvc9ch/nlp_research/VLLM-math-generation/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
Epoch 3/10:   7%|‚ñã         | 1/14 [04:57<1:04:29, 297.65s/it]step:16 - train/loss:0.6913978457450867 - train/lr(1e-3):0.0014285714285714286 - train/time(s):145.38085985183716 - train/avg_response_length:2975.89453125 - train/response_tokens_per_step:761829.0
/u/fvc9ch/nlp_research/VLLM-math-generation/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/u/fvc9ch/nlp_research/VLLM-math-generation/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:659: UserWarning: pin_memory_device is deprecated, the current accelerator will be used as the device,ignore pin_memory_device='cuda'.
  warnings.warn(
step:16 - val/loss:0.6990393400192261 - val/avg_generated_length:2002.4
Epoch 3/10:  14%|‚ñà‚ñç        | 2/14 [13:52<1:27:27, 437.25s/it]step:17 - train/loss:0.7031803727149963 - train/lr(1e-3):0.0021428571428571425 - train/time(s):145.14036083221436 - train/avg_response_length:2805.09375 - train/response_tokens_per_step:718104.0
step:17 - val/loss:0.6988226175308228 - val/avg_generated_length:705.0
Epoch 3/10:  21%|‚ñà‚ñà‚ñè       | 3/14 [18:30<1:06:50, 364.63s/it]step:18 - train/loss:0.6850864887237549 - train/lr(1e-3):0.002857142857142857 - train/time(s):146.27985501289368 - train/avg_response_length:3009.9296875 - train/response_tokens_per_step:770542.0
step:18 - val/loss:0.6983229517936707 - val/avg_generated_length:767.6
Epoch 3/10:  29%|‚ñà‚ñà‚ñä       | 4/14 [23:22<55:56, 335.66s/it]  step:19 - train/loss:0.6881125569343567 - train/lr(1e-3):0.0035714285714285718 - train/time(s):146.32930636405945 - train/avg_response_length:3061.84765625 - train/response_tokens_per_step:783833.0
step:19 - val/loss:0.6976994276046753 - val/avg_generated_length:563.0
Epoch 3/10:  36%|‚ñà‚ñà‚ñà‚ñå      | 5/14 [27:34<45:52, 305.82s/it]step:20 - train/loss:0.7014616131782532 - train/lr(1e-3):0.004285714285714285 - train/time(s):144.67608261108398 - train/avg_response_length:2751.26953125 - train/response_tokens_per_step:704325.0
step:20 - val/loss:0.6970245838165283 - val/avg_generated_length:689.4
Epoch 3/10:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6/14 [32:05<39:10, 293.77s/it]step:21 - train/loss:0.7021912336349487 - train/lr(1e-3):0.005 - train/time(s):145.62077498435974 - train/avg_response_length:2851.2578125 - train/response_tokens_per_step:729922.0
step:21 - val/loss:0.6949818134307861 - val/avg_generated_length:834.8
Epoch 3/10:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7/14 [37:11<34:44, 297.76s/it]step:22 - train/loss:0.6856634616851807 - train/lr(1e-3):0.005714285714285714 - train/time(s):144.53241205215454 - train/avg_response_length:2835.77734375 - train/response_tokens_per_step:725959.0
This is a friendly reminder - the current text generation call has exceeded the model's predefined maximum length (16384). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
step:22 - val/loss:0.6928452253341675 - val/avg_generated_length:3607.2
Epoch 3/10:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8/14 [51:43<48:04, 480.67s/it]step:23 - train/loss:0.6858407855033875 - train/lr(1e-3):0.006428571428571429 - train/time(s):144.97804594039917 - train/avg_response_length:2880.70703125 - train/response_tokens_per_step:737461.0
step:23 - val/loss:0.6905844807624817 - val/avg_generated_length:686.6
Epoch 3/10:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9/14 [56:16<34:38, 415.67s/it]step:24 - train/loss:0.6844459772109985 - train/lr(1e-3):0.0071428571428571435 - train/time(s):146.23231959342957 - train/avg_response_length:2971.51953125 - train/response_tokens_per_step:760709.0
step:24 - val/loss:0.6881670951843262 - val/avg_generated_length:923.6
Epoch 3/10:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10/14 [1:01:27<25:33, 383.29s/it]step:25 - train/loss:0.6938174962997437 - train/lr(1e-3):0.007857142857142858 - train/time(s):145.48382472991943 - train/avg_response_length:2844.25390625 - train/response_tokens_per_step:728129.0
step:25 - val/loss:0.6856257319450378 - val/avg_generated_length:1421.8
Epoch 3/10:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 11/14 [1:08:23<19:40, 393.40s/it]step:26 - train/loss:0.6845388412475586 - train/lr(1e-3):0.00857142857142857 - train/time(s):144.12915182113647 - train/avg_response_length:2679.9296875 - train/response_tokens_per_step:686062.0
step:26 - val/loss:0.6794822216033936 - val/avg_generated_length:3610.0
Epoch 3/10:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12/14 [1:22:56<17:58, 539.28s/it]step:27 - train/loss:0.6908920407295227 - train/lr(1e-3):0.009285714285714288 - train/time(s):144.46962356567383 - train/avg_response_length:2677.78125 - train/response_tokens_per_step:685512.0
step:27 - val/loss:0.672765851020813 - val/avg_generated_length:668.4
Epoch 3/10:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 13/14 [1:27:30<07:38, 458.99s/it]step:28 - train/loss:0.6670694947242737 - train/lr(1e-3):0.01 - train/time(s):144.7140827178955 - train/avg_response_length:2777.35546875 - train/response_tokens_per_step:711003.0
step:28 - val/loss:0.6662018895149231 - val/avg_generated_length:854.6
Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [1:32:00<00:00, 401.93s/it]Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [1:32:01<00:00, 394.38s/it]
Epoch 4/10:   0%|          | 0/14 [00:00<?, ?it/s]step:29 - train/loss:0.6635945439338684 - train/lr(1e-3):0.009998445910004082 - train/time(s):152.8747169971466 - train/avg_response_length:2877.421875 - train/response_tokens_per_step:736620.0
step:29 - val/loss:0.6598507761955261 - val/avg_generated_length:650.2
Epoch 4/10:   7%|‚ñã         | 1/14 [04:51<1:03:14, 291.86s/it]step:30 - train/loss:0.6659428477287292 - train/lr(1e-3):0.009993784606094612 - train/time(s):145.69033479690552 - train/avg_response_length:2847.3203125 - train/response_tokens_per_step:728914.0
step:30 - val/loss:0.6539098620414734 - val/avg_generated_length:831.8
Epoch 4/10:  14%|‚ñà‚ñç        | 2/14 [09:54<59:35, 297.94s/it]  step:31 - train/loss:0.6509528160095215 - train/lr(1e-3):0.009986018985905901 - train/time(s):145.49192881584167 - train/avg_response_length:2838.7578125 - train/response_tokens_per_step:726722.0
step:31 - val/loss:0.6482881307601929 - val/avg_generated_length:3789.4
Epoch 4/10:  21%|‚ñà‚ñà‚ñè       | 3/14 [24:28<1:42:53, 561.24s/it]step:32 - train/loss:0.6533331871032715 - train/lr(1e-3):0.009975153876827007 - train/time(s):144.7246265411377 - train/avg_response_length:2832.02734375 - train/response_tokens_per_step:724999.0
step:32 - val/loss:0.6428776979446411 - val/avg_generated_length:6880.0
Epoch 4/10:  29%|‚ñà‚ñà‚ñä       | 4/14 [39:00<1:53:59, 683.93s/it]step:33 - train/loss:0.6427494287490845 - train/lr(1e-3):0.009961196033000863 - train/time(s):145.31015729904175 - train/avg_response_length:2903.41796875 - train/response_tokens_per_step:743275.0
step:33 - val/loss:0.6377283930778503 - val/avg_generated_length:4330.8
Epoch 4/10:  36%|‚ñà‚ñà‚ñà‚ñå      | 5/14 [53:32<1:52:44, 751.59s/it]step:34 - train/loss:0.6386324167251587 - train/lr(1e-3):0.009944154131125644 - train/time(s):144.32895493507385 - train/avg_response_length:2727.1640625 - train/response_tokens_per_step:698154.0
step:34 - val/loss:0.6327286958694458 - val/avg_generated_length:4712.4
Epoch 4/10:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6/14 [1:08:05<1:45:43, 792.88s/it]step:35 - train/loss:0.6226739883422852 - train/lr(1e-3):0.009924038765061042 - train/time(s):145.24011135101318 - train/avg_response_length:2866.7734375 - train/response_tokens_per_step:733894.0
step:35 - val/loss:0.6279169917106628 - val/avg_generated_length:4037.6
Epoch 4/10:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7/14 [1:22:38<1:35:33, 819.08s/it]step:36 - train/loss:0.629190981388092 - train/lr(1e-3):0.009900862439242718 - train/time(s):145.75786089897156 - train/avg_response_length:2992.5234375 - train/response_tokens_per_step:766086.0
step:36 - val/loss:0.623370349407196 - val/avg_generated_length:6908.2
Epoch 4/10:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8/14 [1:37:13<1:23:41, 836.98s/it]step:37 - train/loss:0.6332949995994568 - train/lr(1e-3):0.009874639560909117 - train/time(s):145.04490542411804 - train/avg_response_length:2878.0859375 - train/response_tokens_per_step:736790.0
step:37 - val/loss:0.6190724968910217 - val/avg_generated_length:4092.4
Epoch 4/10:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9/14 [1:51:46<1:10:41, 848.24s/it]step:38 - train/loss:0.6207845211029053 - train/lr(1e-3):0.00984538643114539 - train/time(s):144.76038789749146 - train/avg_response_length:2799.6875 - train/response_tokens_per_step:716720.0
step:38 - val/loss:0.6148464679718018 - val/avg_generated_length:7424.2
Epoch 4/10:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10/14 [2:06:19<57:03, 855.92s/it] step:39 - train/loss:0.6088424921035767 - train/lr(1e-3):0.00981312123475006 - train/time(s):146.0014295578003 - train/avg_response_length:2970.72265625 - train/response_tokens_per_step:760505.0
step:39 - val/loss:0.6108602285385132 - val/avg_generated_length:4309.4
Epoch 4/10:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 11/14 [2:20:53<43:04, 861.48s/it]step:40 - train/loss:0.5977954864501953 - train/lr(1e-3):0.009777864028930706 - train/time(s):144.74322032928467 - train/avg_response_length:2772.73828125 - train/response_tokens_per_step:709821.0
step:40 - val/loss:0.6070125102996826 - val/avg_generated_length:4070.0
Epoch 4/10:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12/14 [2:35:27<28:50, 865.21s/it]step:41 - train/loss:0.6120108366012573 - train/lr(1e-3):0.00973963673083566 - train/time(s):144.3473687171936 - train/avg_response_length:2717.33984375 - train/response_tokens_per_step:695639.0
step:41 - val/loss:0.6032515168190002 - val/avg_generated_length:6994.2
Epoch 4/10:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 13/14 [2:49:58<14:26, 866.88s/it]step:42 - train/loss:0.6062060594558716 - train/lr(1e-3):0.009698463103929543 - train/time(s):146.0131869316101 - train/avg_response_length:2936.7578125 - train/response_tokens_per_step:751810.0
step:42 - val/loss:0.5995211005210876 - val/avg_generated_length:3895.0
Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [3:04:29<00:00, 868.10s/it]Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [3:04:29<00:00, 790.68s/it]
Epoch 5/10:   0%|          | 0/14 [00:00<?, ?it/s]step:43 - train/loss:0.613547682762146 - train/lr(1e-3):0.009654368743221023 - train/time(s):152.56373381614685 - train/avg_response_length:2810.0859375 - train/response_tokens_per_step:719382.0
step:43 - val/loss:0.5959863066673279 - val/avg_generated_length:10123.6
Epoch 5/10:   7%|‚ñã         | 1/14 [14:54<3:13:44, 894.20s/it]step:44 - train/loss:0.6056752800941467 - train/lr(1e-3):0.009607381059352039 - train/time(s):145.6444115638733 - train/avg_response_length:2909.1796875 - train/response_tokens_per_step:744750.0
step:44 - val/loss:0.5925608277320862 - val/avg_generated_length:10076.2
Epoch 5/10:  14%|‚ñà‚ñç        | 2/14 [29:27<2:56:25, 882.09s/it]step:45 - train/loss:0.5914286375045776 - train/lr(1e-3):0.009557529261558367 - train/time(s):144.69850254058838 - train/avg_response_length:2795.203125 - train/response_tokens_per_step:715572.0
step:45 - val/loss:0.5892724394798279 - val/avg_generated_length:16384.0
Epoch 5/10:  21%|‚ñà‚ñà‚ñè       | 3/14 [43:58<2:40:46, 877.00s/it]step:46 - train/loss:0.5892248749732971 - train/lr(1e-3):0.009504844339512096 - train/time(s):145.95639824867249 - train/avg_response_length:2912.36328125 - train/response_tokens_per_step:745565.0
step:46 - val/loss:0.5860891342163086 - val/avg_generated_length:16384.0
Epoch 5/10:  29%|‚ñà‚ñà‚ñä       | 4/14 [58:33<2:25:59, 875.99s/it]step:47 - train/loss:0.5782068967819214 - train/lr(1e-3):0.009449359044057344 - train/time(s):145.12174916267395 - train/avg_response_length:2840.71484375 - train/response_tokens_per_step:727223.0
step:47 - val/loss:0.58295738697052 - val/avg_generated_length:16384.0
Epoch 5/10:  36%|‚ñà‚ñà‚ñà‚ñå      | 5/14 [1:13:05<2:11:11, 874.64s/it]step:48 - train/loss:0.5821526646614075 - train/lr(1e-3):0.009391107866851142 - train/time(s):145.5056438446045 - train/avg_response_length:2838.58203125 - train/response_tokens_per_step:726677.0
step:48 - val/loss:0.5799866914749146 - val/avg_generated_length:13276.0
Epoch 5/10:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6/14 [1:27:39<1:56:34, 874.28s/it]step:49 - train/loss:0.5785893797874451 - train/lr(1e-3):0.009330127018922196 - train/time(s):145.41640949249268 - train/avg_response_length:2853.48046875 - train/response_tokens_per_step:730491.0
step:49 - val/loss:0.5770381689071655 - val/avg_generated_length:10115.6
Epoch 5/10:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7/14 [1:42:12<1:41:58, 874.07s/it]step:50 - train/loss:0.5763957500457764 - train/lr(1e-3):0.009266454408160778 - train/time(s):144.8181073665619 - train/avg_response_length:2876.40625 - train/response_tokens_per_step:736360.0
step:50 - val/loss:0.5741612911224365 - val/avg_generated_length:10140.4
Epoch 5/10:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8/14 [1:56:44<1:27:20, 873.38s/it]step:51 - train/loss:0.5692137479782104 - train/lr(1e-3):0.009200129615753859 - train/time(s):145.5187385082245 - train/avg_response_length:2898.65234375 - train/response_tokens_per_step:742055.0
step:51 - val/loss:0.5714054703712463 - val/avg_generated_length:6923.6
Epoch 5/10:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9/14 [2:11:17<1:12:45, 873.12s/it]step:52 - train/loss:0.5647905468940735 - train/lr(1e-3):0.009131193871579974 - train/time(s):145.08659315109253 - train/avg_response_length:2791.75 - train/response_tokens_per_step:714688.0
